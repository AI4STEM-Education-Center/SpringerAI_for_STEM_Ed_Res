{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ed45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair Automatic Scoring in STEM Education\n",
    "# ----------------------------------------\n",
    "# Activity 1: Bias Analysis in AI Scoring\n",
    "# Activity 2: Fairness in Dataset Curation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37972bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load and Preview the Dataset\n",
    "# For demo, we create a tiny dataset with gender and response/score.\n",
    "# In class, replace with: pd.read_csv('data/sample_datasets/gender_science_responses.csv')\n",
    "data = pd.DataFrame({\n",
    "    \"gender\": [\"male\", \"female\", \"female\", \"male\", \"female\", \"male\"],\n",
    "    \"response\": [\n",
    "        \"Gravity pulls the apple down.\",\n",
    "        \"Heat causes ice to melt into water.\",\n",
    "        \"Clouds form rain when they cool.\",\n",
    "        \"A plant makes food using sunlight.\",\n",
    "        \"Water boils at 100 degrees Celsius.\",\n",
    "        \"A magnet attracts metal objects.\"\n",
    "    ],\n",
    "    \"score\": [1, 1, 1, 1, 1, 0],  # 1=Correct, 0=Incorrect (for demo)\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "display(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9336f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocess Data\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['response'], padding=True, truncation=True, max_length=64)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels, train_genders, test_genders = train_test_split(\n",
    "    data['response'].tolist(), data['score'].tolist(), data['gender'].tolist(), test_size=0.33, random_state=42, stratify=data['gender']\n",
    ")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=64)\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "test_dataset = SimpleDataset(test_encodings, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train/Fine-tune BERT for Automatic Scoring\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_fair',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e852f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Bias/Fairness Analysis\n",
    "\n",
    "# Predict on test set\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "test_genders = list(test_genders)  # Ensure indexable\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# Compute accuracy for each gender (Scoring Accuracy Difference)\n",
    "results = pd.DataFrame({\n",
    "    \"gender\": test_genders,\n",
    "    \"true_score\": test_labels,\n",
    "    \"pred_score\": pred_labels\n",
    "})\n",
    "\n",
    "acc_by_gender = results.groupby(\"gender\").apply(lambda df: accuracy_score(df[\"true_score\"], df[\"pred_score\"]))\n",
    "print(\"\\nAccuracy by gender:\")\n",
    "print(acc_by_gender)\n",
    "\n",
    "diff = abs(acc_by_gender.max() - acc_by_gender.min())\n",
    "print(f\"\\nScoring Accuracy Difference between genders: {diff:.2f}\")\n",
    "\n",
    "# Mean Score Gaps (MSG) by Gender\n",
    "results[\"score_gap\"] = results[\"pred_score\"] - results[\"true_score\"]\n",
    "msg_by_gender = results.groupby(\"gender\")[\"score_gap\"].mean()\n",
    "print(\"\\nMean Score Gap (AI-human) by gender:\")\n",
    "print(msg_by_gender)\n",
    "\n",
    "# Equalized Odds (Simple version): \n",
    "# For demo: Check if model's error rates are similar for each gender.\n",
    "def false_positive_rate(df):\n",
    "    negatives = df[df[\"true_score\"] == 0]\n",
    "    if len(negatives) == 0: return np.nan\n",
    "    return np.mean(negatives[\"pred_score\"] == 1)\n",
    "\n",
    "def false_negative_rate(df):\n",
    "    positives = df[df[\"true_score\"] == 1]\n",
    "    if len(positives) == 0: return np.nan\n",
    "    return np.mean(positives[\"pred_score\"] == 0)\n",
    "\n",
    "eo = results.groupby(\"gender\").apply(lambda df: pd.Series({\n",
    "    \"FPR\": false_positive_rate(df),\n",
    "    \"FNR\": false_negative_rate(df)\n",
    "}))\n",
    "print(\"\\nApproximate Equalized Odds (error rates) by gender:\")\n",
    "print(eo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Activity: Curate and Test a More Diverse Dataset\n",
    "\n",
    "print(\"\\nNow, try editing/adding examples below to make the dataset more balanced and diverse, then rerun the analysis above.\")\n",
    "\n",
    "# For demo, create or ask students to edit this cell:\n",
    "data2 = pd.DataFrame({\n",
    "    \"gender\": [\"male\", \"female\", \"female\", \"male\", \"female\", \"male\", \"nonbinary\"],\n",
    "    \"response\": [\n",
    "        \"Gravity pulls objects toward Earth.\",\n",
    "        \"Ice becomes water when heated.\",\n",
    "        \"Plants make oxygen.\",\n",
    "        \"A compass points north due to magnetism.\",\n",
    "        \"Sound travels through air.\",\n",
    "        \"Metal expands when heated.\",\n",
    "        \"Friction slows moving objects.\"\n",
    "    ],\n",
    "    \"score\": [1, 1, 1, 1, 1, 0, 1],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4ced9",
   "metadata": {},
   "source": [
    "## Reflection: Fairness and Bias in AI Scoring\n",
    "\n",
    "- Did the model's scoring accuracy differ by gender or other groups?\n",
    "- Were the Mean Score Gaps or error rates (FPR/FNR) noticeably higher for some groups?\n",
    "- How did adding or balancing data change the fairness metrics?\n",
    "- What steps can we take to reduce bias in automatic scoring systems?\n",
    "- Why is it important for educational AI to be fair and unbiased?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
