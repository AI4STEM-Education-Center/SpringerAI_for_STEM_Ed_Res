{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc30c937",
   "metadata": {},
   "source": [
    "# Contextualized Automatic Scoring in STEM Education\n",
    "# --------------------------------------------------\n",
    "# Activity 1: Context-Based Science Assessment\n",
    "# Activity 2: Peer Review and Contextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed155eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6078b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5771b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load and Preview the Dataset\n",
    "# For demo: We'll create a tiny toy dataset.\n",
    "# In class, replace this with: pd.read_csv('data/sample_datasets/science_responses.csv')\n",
    "data = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"Explain what happens when ice melts.\",\n",
    "        \"Why does a ball fall to the ground?\",\n",
    "        \"Describe what causes rain.\",\n",
    "        \"How does a plant make food?\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"Ice turns into water because of heat.\",\n",
    "        \"The ball falls because of gravity.\",\n",
    "        \"Rain happens when clouds cool and water falls.\",\n",
    "        \"A plant uses sunlight to make food by photosynthesis.\"\n",
    "    ],\n",
    "    \"score\": [1, 1, 1, 1],  # 1=Correct, 0=Incorrect (binary for demo)\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Preprocess Data\n",
    "# We'll use the 'response' as input and 'score' as label.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['response'], padding=True, truncation=True, max_length=64)\n",
    "\n",
    "# For a real dataset, split train/test.\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['response'].tolist(), data['score'].tolist(), test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=64)\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "test_dataset = SimpleDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cd49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fine-tune BERT for Scoring (Binary Classification)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"Training model (this may take a few minutes on CPU)...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the Model for Automated Scoring\n",
    "\n",
    "# Predict on test data\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d32aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Activity: Try Your Own Response\n",
    "\n",
    "def score_response(your_response):\n",
    "    inputs = tokenizer([your_response], truncation=True, padding=True, max_length=64, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "    if pred == 1:\n",
    "        return \"Correct / On Target\"\n",
    "    else:\n",
    "        return \"Needs Improvement\"\n",
    "\n",
    "print(\"\\nTry scoring your own response:\")\n",
    "user_input = input(\"Enter a science answer to score (e.g., 'Plants use sunlight to make food.'):\\n\")\n",
    "print(\"AI Score:\", score_response(user_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Activity 2: Peer Review and Contextualization\n",
    "\n",
    "print(\"\\nPeer Review Activity:\")\n",
    "print(\"1. In pairs or groups, exchange your written science responses.\")\n",
    "print(\"2. Each partner enters their peer's response and notes the AI's feedback.\")\n",
    "print(\"3. Compare the AI's score with your own judgment as a human reviewer.\")\n",
    "print(\"4. Discuss: Did the model miss context or nuance? How could the feedback be improved?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c76b8",
   "metadata": {},
   "source": [
    "## Reflection: Contextualization in AI Scoring\n",
    "\n",
    "- Did the AI model give appropriate feedback?\n",
    "- Were there cases where the model misunderstood the scientific context?\n",
    "- How did the AI feedback compare to your peer's/human feedback?\n",
    "- What could be improved in this scoring system?\n",
    "- Why is understanding context important in STEM assessment?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
