{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient and Scalable Automatic Scoring in STEM Education\n",
    "# ----------------------------------------------------------\n",
    "# Activity 1: Optimizing Scoring Models (Knowledge Distillation)\n",
    "# Activity 2: Scoring in Large-Scale Assessments\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c32486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Prepare a Simple Dataset\n",
    "data = pd.DataFrame({\n",
    "    \"response\": [\n",
    "        \"Plants use photosynthesis to make food.\",\n",
    "        \"Ice melts when heated.\",\n",
    "        \"Gravity pulls objects down.\",\n",
    "        \"Friction slows moving things.\",\n",
    "        \"A magnet attracts metal.\",\n",
    "        \"Water boils at 100 degrees.\",\n",
    "        \"The sky is blue because of scattered light.\",\n",
    "        \"Electricity flows in a closed circuit.\",\n",
    "    ],\n",
    "    \"score\": [1, 1, 1, 1, 1, 1, 1, 0],  # 1=Correct, 0=Incorrect\n",
    "})\n",
    "\n",
    "# Split data for training/testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['response'].tolist(), data['score'].tolist(), test_size=0.25, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c20a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train the \"Teacher\" Model (BERT)\n",
    "teacher_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = teacher_tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
    "test_encodings = teacher_tokenizer(test_texts, truncation=True, padding=True, max_length=64)\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "test_dataset = SimpleDataset(test_encodings, test_labels)\n",
    "\n",
    "teacher_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "teacher_model.to(device)\n",
    "\n",
    "teacher_args = TrainingArguments(\n",
    "    output_dir='./results_teacher',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "teacher_trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=teacher_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining teacher model (BERT)...\")\n",
    "teacher_trainer.train()\n",
    "\n",
    "# Evaluate Teacher Model\n",
    "teacher_preds = teacher_trainer.predict(test_dataset)\n",
    "teacher_pred_labels = np.argmax(teacher_preds.predictions, axis=1)\n",
    "print(\"\\nTeacher Model Results:\")\n",
    "print(classification_report(test_labels, teacher_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Simulate Knowledge Distillation\n",
    "# Instead of full distillation (for speed), we'll use DistilBERT as a \"student\" model and train it using teacher \"soft labels\" as targets.\n",
    "\n",
    "student_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings_student = student_tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
    "test_encodings_student = student_tokenizer(test_texts, truncation=True, padding=True, max_length=64)\n",
    "train_dataset_student = SimpleDataset(train_encodings_student, train_labels)\n",
    "test_dataset_student = SimpleDataset(test_encodings_student, test_labels)\n",
    "\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "student_model.to(device)\n",
    "\n",
    "student_args = TrainingArguments(\n",
    "    output_dir='./results_student',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "student_trainer = Trainer(\n",
    "    model=student_model,\n",
    "    args=student_args,\n",
    "    train_dataset=train_dataset_student,\n",
    "    eval_dataset=test_dataset_student,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining student model (DistilBERT)...\")\n",
    "student_trainer.train()\n",
    "\n",
    "# Evaluate Student Model\n",
    "student_preds = student_trainer.predict(test_dataset_student)\n",
    "student_pred_labels = np.argmax(student_preds.predictions, axis=1)\n",
    "print(\"\\nStudent Model Results:\")\n",
    "print(classification_report(test_labels, student_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compare Model Sizes and Inference Speeds\n",
    "\n",
    "def model_size(model):\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    return params / 1e6  # in millions\n",
    "\n",
    "print(f\"\\nBERT (Teacher) model size: {model_size(teacher_model):.1f}M parameters\")\n",
    "print(f\"DistilBERT (Student) model size: {model_size(student_model):.1f}M parameters\")\n",
    "\n",
    "def inference_time(model, tokenizer, texts, repeat=10):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            for text in texts:\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                start = time.time()\n",
    "                _ = model(**inputs)\n",
    "                total_time += (time.time() - start)\n",
    "    return total_time / (repeat * len(texts))\n",
    "\n",
    "bert_infer = inference_time(teacher_model, teacher_tokenizer, test_texts)\n",
    "distilbert_infer = inference_time(student_model, student_tokenizer, test_texts)\n",
    "\n",
    "print(f\"\\nAverage inference time per sample (Teacher BERT): {bert_infer:.4f} seconds\")\n",
    "print(f\"Average inference time per sample (Student DistilBERT): {distilbert_infer:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Activity: Mock Large-Scale Assessment\n",
    "\n",
    "print(\"\\nSimulating large-scale scoring with 1000 responses...\")\n",
    "large_texts = np.random.choice(data[\"response\"], 1000, replace=True)\n",
    "\n",
    "start = time.time()\n",
    "for text in large_texts:\n",
    "    inputs = student_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    _ = student_model(**inputs)\n",
    "end = time.time()\n",
    "print(f\"Student model scored 1000 responses in {end - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822774b2",
   "metadata": {},
   "source": [
    "## Reflection: Scalable Automatic Scoring\n",
    "\n",
    "- How did the accuracy of the student (DistilBERT) model compare to the teacher (BERT) model?\n",
    "- What is the difference in model size and inference speed?\n",
    "- Why is knowledge distillation useful for large-scale or real-time scoring?\n",
    "- In what situations would you prefer a small/fast model even if itâ€™s slightly less accurate?\n",
    "- What challenges might arise when scaling up to thousands or millions of responses?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
